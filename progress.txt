# RocksDB Benchmark Progress Log

## Initial Assessment (2026-01-13)

### Current State:
- Binaries built: target/release/reth and target/release/reth-bench exist
- RocksDB feature: ENABLED (--storage.*-in-rocksdb flags available)
- Existing Hoodi datadir: /home/yk/.local/share/reth/hoodi
  - Block height: 1,766,369
  - Has rocksdb/ subdirectory already
  - JWT secret available at jwt.hex
- Environment variables: NOT SET (need to configure)
- Benchmark results directory: EXISTS (empty)

### Next Steps:
1. Set up environment variables
2. Create isolated copies for MDBX and RocksDB benchmarks
3. Determine START_BLOCK and RPC_URL

---

## Story 1 COMPLETED (2026-01-13)

### Setup Benchmark Environment - PASSED

**Verified:**
- ✅ Environment variables configured in `benchmark-env.sh`:
  - HOODI_DATADIR_MDBX=/home/yk/.local/share/reth/hoodi-bench-mdbx
  - HOODI_DATADIR_ROCKS=/home/yk/.local/share/reth/hoodi-bench-rocks  
  - RPC_URL=http://localhost:8545
  - START_BLOCK=1765369
- ✅ Isolated directories created (copied from source hoodi datadir)
- ✅ RocksDB feature enabled (4 rocksdb flags available in reth binary)
- ✅ benchmark-results directory structure exists (charts/, csv/, logs/)

### Next: Story 2 - Prepare MDBX baseline node
Need to unwind MDBX node to START_BLOCK (1765369) and verify metrics endpoint.

---

## Story 2 BLOCKED (2026-01-13)

### Prepare MDBX Baseline Node - BLOCKED

**Attempted:**
- Tried to unwind MDBX node to START_BLOCK (1765369) - FAILED
- Tried unwinding smaller amounts (100, 50, 10 blocks) - ALL FAILED
- Tried unwinding with RocksDB flags matching storage settings - FAILED

**Root Cause Analysis:**

The database has the following storage settings:
```
storages_history_in_rocksdb: true
account_history_in_rocksdb: true
transaction_hash_numbers_in_rocksdb: false
```

The unwind operation fails with **state root mismatch** at the MerkleUnwind stage.

**Error Pattern:**
```
stage encountered an error in block #1766359: validation error: mismatched block state root
got: 0x0eba72316f0aba5dfef2f0b26350a12b60294751f851b2bb74c9e133f2019358
expected: 0x98ea9451ebd090fd669720ccabaf12179ac296f15576e7b331a4a93095051451
```

**Additional Findings:**
1. `AccountsTrieChangeSets` and `StoragesTrieChangeSets` tables are EMPTY (0 entries)
2. Config has `merkle_changesets: Distance(128)` pruning enabled
3. Unwind fails even within 128-block distance
4. This appears to be a bug in the RocksDB/MDBX hybrid unwind logic

**Blocker:**
Cannot proceed with benchmark execution until the unwind functionality is fixed.
The state root computed during Merkle unwind does not match the expected header state root,
suggesting either database corruption or a bug in the unwind process with RocksDB-backed tables.

**Recommendations:**
1. File a bug report with reth team
2. Investigate if there's an issue with how RocksDB history tables interact with Merkle unwind
3. Consider using a freshly synced node without RocksDB history for benchmarking
4. Alternative: Skip unwind and benchmark from current head (no warm-up phase)

---

## Investigation Findings (2026-01-13, continued)

### Root Cause Identified: Database State Inconsistency

**Stage Checkpoint Analysis:**
```
StageCheckpoints:
- MerkleChangeSets: block 0 (never populated!)
- MerkleExecute: block 1,764,300
- Execution: block 1,766,369
```

The database has a 2,069 block gap between MerkleExecute and Execution stages!

**Key Findings:**
1. `MerkleChangeSets` stage was NEVER executed (checkpoint at block 0)
2. `AccountsTrieChangeSets` and `StoragesTrieChangeSets` are empty (0 entries)
3. `MerkleExecute` is at 1,764,300 but Execution is at 1,766,369

**Why Unwind Fails:**
The unwind needs to compute the state root after reverting changesets. However:
- The trie changesets are missing (never generated)
- MerkleExecute checkpoint doesn't match actual database state
- The incremental state root calculation fails because it can't properly
  reverse the trie changes without the changeset data

**Resolution Options:**
1. **Re-sync from scratch**: Create a new database with all stages properly executed
2. **Run MerkleChangeSets stage**: Execute `reth stage run merkle-changesets` to populate missing data
3. **Skip unwind-based benchmarks**: Run benchmarks from current head instead of unwinding

**Note:** This is NOT a RocksDB-specific bug. It's a database consistency issue where
the MerkleChangeSets stage was never run, likely because the node was synced before
this stage was added or with an older configuration.

---

## New Approach (2026-01-13)

### Decision: Benchmark from Current Head

Since unwind doesn't work with this database state, we'll benchmark by:
1. Starting the node from current head (1,766,369)
2. Let it sync new blocks from the network
3. Capture performance metrics during live sync

### Benchmark Order:
1. **RocksDB First**: The database already has RocksDB history tables enabled
   - Run node with RocksDB flags
   - Capture metrics for 1000+ blocks of live sync

2. **MDBX After**: For MDBX baseline, need to either:
   - Option A: Drop RocksDB tables and run without RocksDB flags
   - Option B: Use a fresh MDBX-only sync (longer but cleaner)

### Current Storage Settings (confirmed):
```
storages_history_in_rocksdb: true
account_history_in_rocksdb: true
transaction_hash_numbers_in_rocksdb: false
```

---

## Complete Root Cause Analysis (2026-01-13)

### Why Unwind Fails

**Stage Checkpoint Status:**
```
Stage                 | Block Number | Progress
----------------------|--------------|------------------
Execution             | 1,766,369    | Complete
AccountHashing        | 1,764,300    | 15,099,797 / 15,100,467 (INCOMPLETE!)
StorageHashing        | 1,764,300    | 210,580,462 / 210,841,953 (INCOMPLETE!)
MerkleExecute         | 1,764,300    | -
MerkleChangeSets      | 0            | Never run
```

**The Problem:**
1. Execution stage is at block 1,766,369 (fully synced)
2. Hashing stages are stuck at 1,764,300 with incomplete progress
   - 670 accounts NOT hashed
   - 261,491 storage slots NOT hashed
3. MerkleExecute is at 1,764,300 (follows hashing stages)
4. MerkleChangeSets was never executed (checkpoint at 0)

**Why This Breaks Unwind:**
- Unwind needs to compute state root after reverting changes
- State root computation requires:
  1. Complete HashedAccounts table
  2. Complete HashedStorages table
  3. Consistent trie data (AccountsTrie, StoragesTrie)
- Since hashing is incomplete, the trie doesn't match the actual state
- Result: Computed state root ≠ Expected state root → unwind fails

**Attempts to Fix:**
1. Running `stage run merkle` → Fails with state root mismatch
2. Running `stage run hashing` → No effect, checkpoint not updated
3. Running `stage run account-hashing` → No effect

**Conclusion:**
The database is in an inconsistent state that cannot be easily fixed.
The pipeline was interrupted or had issues between blocks 1,764,300 and 1,766,369.

### Options to Proceed

1. **Re-sync from scratch** (cleanest, slowest)
   - Delete database, sync fresh with all stages working

2. **Sync forward** (use current state)
   - Start node from current head
   - Let it sync new blocks
   - Benchmark during live sync

3. **Debug and fix** (complex)
   - Would require deep investigation into why stages got out of sync
   - Potentially hours of debugging

---

## Fresh Sync Attempt (2026-01-13)

### Blocker: No Beacon Node Available

Attempted to start a fresh sync with:
```bash
reth node --chain hoodi --datadir /home/yk/.local/share/reth/hoodi-fresh-bench \
  --storage.account-history-in-rocksdb --storage.storages-history-in-rocksdb \
  --http --metrics 127.0.0.1:9001 --full
```

**Result:** Node starts, connects to peers, but does NOT sync.

**Reason:** Hoodi is a post-merge network. Reth requires a beacon node (consensus client)
to provide Engine API calls (forkchoice updates) to sync. Without it, the node just waits.

**Findings:**
- Lighthouse data exists at `/home/yk/.lighthouse/hoodi/beacon`
- But lighthouse binary not found in PATH or common locations

**Options:**
1. Install lighthouse beacon node
2. Find existing lighthouse binary
3. Use checkpoint sync if available
4. Fall back to existing database (with issues)

